{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886cecde",
   "metadata": {},
   "source": [
    "# 선형회귀(Numpy)\n",
    "\n",
    "> 선형 회귀는 독립 변수(x)와 종속 변수(y) 간의 선형 관계를 모델링해서 \"예측\"하는 통계 기법입니다\n",
    "\n",
    "## 모델\n",
    "\n",
    "- 단순 모델: y = ax + b\n",
    "- 다중 모델: y = a1x1 + a2x2 + ... + anxn + b\n",
    "\n",
    "## 머신러닝 활용\n",
    "\n",
    " - y와 x가 결정 => {a,b} 매개변수를 찾아야 함\n",
    " - y와 x가 선형 관계를 가져야 함, 이상치와 다중공선성\n",
    "\n",
    " ## {a,b} 매개변수를 찾아야 함!\n",
    " \n",
    " - 매개변수를 찾는 방법: 최소제곱법, 경사하강법\n",
    " - 평가는 : MSE, R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4922eb8",
   "metadata": {},
   "source": [
    "## Numpy를 활용한 선형회귀 모델(v2)\n",
    "\n",
    "> pip install scikit-learn 설치하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d21544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e744d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df[\"target\"] = iris.target\n",
    "# iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2db4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 그래프를 보고 전처리를 해야함\n",
    "# sns.pairplot(iris_df, hue=\"target\")\n",
    "# plt.suptitle(\"Iris Data Pair Plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b024e",
   "metadata": {},
   "source": [
    "### 학습용 데이터(X, y)로 데이터 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94fcfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([iris.data[:, 0], \n",
    "                     iris.data[:, 2], \n",
    "                     iris.data[:, 3]])\n",
    "y = iris.data[:, 1]\n",
    "#print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff8ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용/검증용으로 데이터 나누기\n",
    "def custom_train_test_split(X, y, test_size=0.3, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    indices = np.random.permutation(len(X))\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f59ead33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 3) (45, 3) (105,) (45,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(X, y)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf50272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최소 제곱법\n",
    "def fit_linear_regreesion_lsq(X, y):\n",
    "    X_bias = np.column_stack([np.ones(len(X)), X]) # 매개변수 순서 중요!\n",
    "    cofficients, _, _, _ = np.linalg.lstsq(X_bias, y)\n",
    "    intercept_ = cofficients[0]\n",
    "    coef_ = cofficients[1:]\n",
    "    return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "763bd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 경사 하강법\n",
    "#경사하강법의 성능과 동작방식을 결정하는 것은 *하이퍼 파라미터\n",
    "#하이퍼 파라미터 : 모델이 학습을 통해 스스로 찾는 값(파라미터, 예: 가중치 w)이 아니라, 개발자가 직접 설정해줘야 하는 값\n",
    "#경사하강법은 이상치에 민감 -> 이상치를 누르기 위해 정규화부터 해야함\n",
    "\n",
    "def fit_linear_regreesion_gd(X, y, learning_rate=0.01, max_iter = 1000, tolerance = 1e-6):\n",
    "    \"\"\"\n",
    "    learning_rate(학습률) : 기울기를 얼마만큼 이동 시킬지 결정\n",
    "    max_iter(최대이동 횟수) : 기울기를 최대 몇번 이동 시킬지 결정\n",
    "    tolerance(수렴조건) : max_iter번 시행하는 중에 tolerance를 만족하면 멈춤\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    weights = np.random.normal(0, 0.01, n_features+1) #정규화(MSE를 이용해서 비용함수를 계산할 것이라서 이상치에 민감-> 이상치를 누르기 위해 정규화 필요함) #절편도 가져옴\n",
    "    X_with_bias = np.column_stack([np.ones(n_samples),X]) # [절편, 특징1, 특징2, 특징 3]..이 총 150개  # 매개변수의 순서조심 -> 절편이 맨 앞으로 나옴\n",
    "    \n",
    "    cost_history= []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        #1. 예측값 계산\n",
    "        predictions = X_with_bias @ weights # 정규화된 값을 초기값으로 넣게 되면, 그냥 랜덤한 값을 넣는 것보다는 유의미할 것\n",
    "\n",
    "        #2. 비용함수를 계산 -> MSE 이용\n",
    "        cost = np.mean((predictions-y)**2)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        #3. 기울기 계산 -> 편미분 공식을 이용하기 (aj/aw) = (2/n)*(X^T)*(Xw-y)\n",
    "        gradients = (2/n_samples) * X_with_bias.T @ (predictions-y)\n",
    "\n",
    "        #4. 가중치를 업데이트\n",
    "        weights = weights - learning_rate*gradients # 이 과정을 통해서 최적해 기울기를 구할 수 있음\n",
    "\n",
    "        #5. 수렴조건 점검\n",
    "        if i>0 and abs(cost_history[-2] - cost_history[-1])<tolerance:\n",
    "            print(f\"경사하강법이 {i+1}번째 이동에서 수렴했음\")\n",
    "            break\n",
    "\n",
    "    intercept_ = weights[0] \n",
    "    coef_ = weights[1:]\n",
    "\n",
    "    return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cdb1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_regreesion(X, coef_, intercept_):\n",
    "    return X @ coef_ + intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "791ec43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_, intercept_ = fit_linear_regreesion_lsq(X_train, y_train)\n",
    "y_pred_lsq = predict_linear_regreesion(X_test, coef_, intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34c84008",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_, intercept_ = fit_linear_regreesion_gd(X_train, y_train)\n",
    "y_pred_gd = predict_linear_regreesion(X_test, coef_, intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dce971",
   "metadata": {},
   "source": [
    "### 회귀모델 성능평가 지표 ex) MAE, MSE, RMSE .... 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22af63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE(평균제곱오차) : 실제값, 예측값을 매개변수로 받아와야함\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c9c6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2(결정계수) : \n",
    "def r2_score(y_true, y_pred):\n",
    "    # 잔차의 제곱합 = 오차의 제곱합과 동일하게 구함\n",
    "    ss_res = np.sum((y_true-y_pred)**2)\n",
    "\n",
    "    # 총 제곱합\n",
    "    y_mean = np.mean(y_true)\n",
    "    ss_tot = np.sum((y_true-y_mean)**2)\n",
    "\n",
    "    #r2\n",
    "    r2 = 1-(ss_res/ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0847219715794219 0.3810001715675807\n"
     ]
    }
   ],
   "source": [
    "mse_lsq = mean_squared_error(y_test, y_pred_lsq)\n",
    "r2_lsq = r2_score(y_test, y_pred_lsq)\n",
    "print(mse_lsq, r2_lsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ac1bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10689432819723516 0.2190034110282827\n"
     ]
    }
   ],
   "source": [
    "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
    "r2_gd = r2_score(y_test, y_pred_gd)\n",
    "print(mse_gd, r2_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대응 모델()\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(X_train, y_train)\n",
    "y_pred_br = br.predict(X_test)\n",
    "mse_br = mean_squared_error(y_test, y_pred_br)\n",
    "r2_br = r2_score(y_test, y_pred_br)\n",
    "print(mse_br, r2_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: seaborn으로 변경\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "axes[0].scatter(y_test, y_pred_custom, alpha=0.7, color=\"blue\")\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])\n",
    "axes[1].scatter(y_test, y_pred_br, alpha=0.7, color=\"black\")\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc801b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
